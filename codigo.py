# Bibliotecas utilizadas
import pandas as pd
import requests
import json
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from IPython.display import display, HTML

# Leitura dos dados tratados a partir do GitHub
url = 'https://raw.githubusercontent.com/Brunex-Alado/Analise-Dados-3/refs/heads/main/dados_tratados.csv'
df = pd.read_csv(url)

# Visualizar as 5 primeiras linhas
df.head()



# PREPARA√á√ÉO DOS DADOS E REMO√á√ÉO DE COLUNAS IRRELEVANTES:

# DataFrame
dados = df

# Selecionar apenas colunas num√©ricas e booleanas
dados_numericos = dados.select_dtypes(include=["number", "bool"])

# Calcular a matriz de correla√ß√£o absoluta
correlacao = dados_numericos.corr().abs()

# Identificar pares altamente correlacionados (correla√ß√£o > 0.95)
pares_correlacionados = [
    (col1, col2)
    for col1 in correlacao.columns
    for col2 in correlacao.columns
    if col1 != col2 and correlacao.loc[col1, col2] > 0.95
]

# Mostrar os pares encontrados
print("üîÅ PARES DE COLUNAS COM ALTA CORRELA√á√ÉO:")
print('-'*41)
for col1, col2 in pares_correlacionados:
    print(f"{col1}  üîõ  {col2}")

# Sugerir colunas para remo√ß√£o
colunas_para_remover = set()
for col1, col2 in pares_correlacionados:
    colunas_para_remover.add(col2)

# Exibir colunas sugeridas
print("\n‚ùå COLUNAS SUGERIDAS PARA REMO√á√ÉO (por redund√¢ncia):")
print('-'*47)
for col in colunas_para_remover:
    print(f"- {col}")


# Remover as colunas com alta correla√ß√£o
dados = dados.drop(['ID_Cliente', 'tempo_estimado_meses', 'Conta_Diarias', 'Valor_Total'], axis=1)

# Remover registros onde o valor √© 'N√£o informado'
dados = dados[dados['Cancelamento'] != 'N√£o informado']

dados.head()



# ENCODING

# Para saber os valores √∫nicos por coluna e quantidade.
for col in dados.select_dtypes(include='object').columns:
    titulo = f"<strong style='font-size: 15px;'>‚û°Ô∏è  {col.upper()}</strong>"
    display(HTML(titulo))
    display(dados[col].value_counts(dropna=False).head(3).to_frame(name='Frequ√™ncia'))
    print("-" * 35)


# Passo 1: Mapear os valores para bin√°rio
import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore", category=FutureWarning)
    dados = dados.replace({'Yes': 1, 'No': 0})
    dados = dados.replace({'Female': 1, 'Male': 0})
    dados = dados.replace({'True': 1, 'False': 0})

dados.info()


# Passo 2: Identificar colunas que t√™m s√≥ 0 e 1
binarias = [col for col in dados.columns if set(dados[col].dropna().unique()).issubset({0, 1})]

# Passo 3: Converter essas colunas para booleano
dados[binarias] = dados[binarias].astype(bool)

dados.info()


# Passo 4: One-hot encoding para todas as colunas categ√≥ricas multiclasse
dados = pd.get_dummies(dados, drop_first=True)

dados.info()


# Passo 5: Calcular novamente a correla√ß√£o apenas entre colunas num√©ricas
dados_numericos = dados.select_dtypes(include=['number', 'bool'])
correlacao = dados_numericos.corr().abs()

# Pegar pares com alta correla√ß√£o
pares_correlacionados = [
    (col1, col2)
    for col1 in correlacao.columns
    for col2 in correlacao.columns
    if col1 != col2 and correlacao.loc[col1, col2] > 0.95
]

print("üîÅ PARES DE COLUNAS COM ALTA CORRELA√á√ÉO:")
print('-'*40)
for col1, col2 in pares_correlacionados:
    print(f"{col1}  üîõ  {col2}")


# Passo 6: Remover colunas com alta correla√ß√£o:
dados = dados.drop([ 'Multiplas_Linhas_No phone service',
    'Backup_Online_No internet service',
    'Protecao_Dispositivo_No internet service',
    'Suporte_Tecnico_No internet service',
    'Streaming_TV_No internet service',
    'Streaming_Filmes_No internet service'], axis=1)

# Remover registros onde o valor √© 'N√£o informado'
dados = dados[dados['Cancelamento'] != 'N√£o informado']


dados.head()



# VERIFICA√á√ÉO DA PROPOR√á√ÉO DE EVAS√ÉO:

# Frequ√™ncia absoluta e relativa da coluna Cancelamento
frequencia = dados['Cancelamento'].value_counts()
proporcao = dados['Cancelamento'].value_counts(normalize=True) * 100

# Visualizar com gr√°fico de barras
plt.figure(figsize=(10, 6))
sns.barplot(x=proporcao.index, y=proporcao.values, hue=proporcao.index, palette='icefire', legend=False)
plt.ylabel('Propor√ß√£o (%)')
plt.title('Distribui√ß√£o da Evas√£o de Clientes')
plt.show()

# Exibir os resultados
print("\nüìä FREQUENCIA ABSOLUTA:")
print("-" * 23)
print(frequencia)
print("-\n" * 1)
print("üìà PROPOR√á√ÉO (%):")
print("-" * 17)
print(proporcao)
print("-\n")



# BALANCEAMENTO DE CLASSES:

# Separar features(X) e target(y)
X = dados.drop('Cancelamento', axis=1)
y = dados['Cancelamento']


# Oversampling com RandomOverSampler (Duplica registros da classe minorit√°ria)

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=42)
X_res, y_res = ros.fit_resample(X, y)

print("DISTRIBUI√á√ÉO AP√ìS OVERSAMPLING:")
print("-" * 31)
print(pd.Series(y_res).value_counts())


# Undersampling com RandomUnderSampler (Remove registros da classe majorit√°ria)
# Gera novas amostras sint√©ticas da classe minorit√°ria, em vez de apenas copiar.

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42)
X_res, y_res = rus.fit_resample(X, y)

print("DISTRIBUI√á√ÉO AP√ìS UNDERSAMPLING:")
print("-" * 32)
print(pd.Series(y_res).value_counts())


# SMOTE (Synthetic Minority Over-sampling Technique)
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

print("DISTRIBUI√á√ÉO AP√ìS SMOTE:")
print("-" * 24)
print(pd.Series(y_res).value_counts())



#  NORMALIZA√á√ÉO OU PADRONIZA√á√ÉO

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Separar X e y
X = dados.drop(columns=['Cancelamento'])
y = dados['Cancelamento']

# Dividir treino e teste (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Identificar colunas num√©ricas para padronizar
colunas_numericas = ['Meses_de_Contrato', 'Valor_Mensal']

# Criar c√≥pias para n√£o modificar original
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

# Aplicar StandardScaler s√≥ nas colunas num√©ricas
scaler = StandardScaler()
X_train_scaled[colunas_numericas] = scaler.fit_transform(X_train[colunas_numericas])
X_test_scaled[colunas_numericas] = scaler.transform(X_test[colunas_numericas])

# Agora se pode usar X_train_scaled e X_test_scaled para treinar modelos sens√≠veis √† escala.



# CORRELA√á√ÉO E SELE√á√ÉO DE VARI√ÅVEIS

# Identificar as colunas num√©ricas automaticamente
colunas_numericas = dados.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Para garantir que a coluna alvo 'Cancelamento' esteja inclu√≠da
if 'Cancelamento' not in colunas_numericas:
    colunas_numericas.append('Cancelamento')

# Criar um DataFrame apenas com as colunas num√©ricas
dados_numericos = dados[colunas_numericas]


# Calcular matriz de correla√ß√£o
corr = dados_numericos.corr()

# Visualizar matriz de correla√ß√£o
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap='viridis', fmt=".2f", square=True)
plt.title('Matriz de Correla√ß√£o das Vari√°veis Num√©ricas')
plt.show()


# Analisar correla√ß√£o com evas√£o ---
corr_com_alvo = corr['Cancelamento'].drop('Cancelamento').sort_values(ascending=False)


# AN√ÅLISE DIRECIONADA:

# 1 linha, 2 colunas
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Boxplot com hue e legend=False para evitar warning
sns.boxplot(ax=axes[0], x='Cancelamento', y='Meses_de_Contrato', hue='Cancelamento', palette='viridis', data=dados, legend=False)
axes[0].set_title('Tempo de Contrato por Evas√£o',fontsize=12)
axes[0].set_ylabel('Meses de Contrato')

sns.stripplot(ax=axes[1], x='Cancelamento', y='Valor_Mensal', hue='Cancelamento',jitter=True, alpha=0.5, data=dados, legend=False)
axes[1].set_title('Valor Mensal por Evas√£o', fontsize=12)
axes[1].set_ylabel('Valor Mensal')

plt.tight_layout()

# Aumenta o espa√ßo horizontal entre os gr√°ficos
plt.subplots_adjust(hspace=0.5, wspace=0.3)
plt.show()



# MODELAGEM PREDITIVA

# Dividir treino e teste (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)


# CRIA√á√ÉO DE MODELOS:

# MODELO 1: Regress√£o Log√≠stica (com normaliza√ß√£o)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Normaliza√ß√£o apenas para o Modelo 1
scaler = StandardScaler()
X_train_norm = scaler.fit_transform(X_train)
X_test_norm = scaler.transform(X_test)

# Treinar modelo com dados normalizados
modelo_log = LogisticRegression()
modelo_log.fit(X_train_norm, y_train)

# Previs√µes
y_pred_log = modelo_log.predict(X_test_norm)

# Avalia√ß√£o
print("üîç REGRESS√ÉO LOG√çSTICA")
print("-" * 23)
print(confusion_matrix(y_test, y_pred_log))
print(classification_report(y_test, y_pred_log))

# MODELO 2: Random Forest

from sklearn.ensemble import RandomForestClassifier

# Treinar modelo com dados originais
modelo_rf = RandomForestClassifier(random_state=42)
modelo_rf.fit(X_train, y_train)

# Previs√µes
y_pred_rf = modelo_rf.predict(X_test)

# Avalia√ß√£o
print("üå≥ RANDOM FOREST")
print("-" * 17)
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))


# MODELO 3: KNN - K Vizinhos Mais Pr√≥ximos (com normaliza√ß√£o)
from sklearn.neighbors import KNeighborsClassifier

modelo_knn = KNeighborsClassifier(n_neighbors=5)
modelo_knn.fit(X_train_norm, y_train)
y_pred_knn = modelo_knn.predict(X_test_norm)

# Avalia√ß√£o
print("üîç KNN")
print("-" * 10)
print(confusion_matrix(y_test, y_pred_knn))
print(classification_report(y_test, y_pred_knn))


# MODELO 4: √Årvore de Decis√£o (sem normaliza√ß√£o)
from sklearn.tree import DecisionTreeClassifier

modelo_dt = DecisionTreeClassifier(random_state=42)
modelo_dt.fit(X_train, y_train)
y_pred_dt = modelo_dt.predict(X_test)

# Avalia√ß√£o
print("üå≥ √ÅRVORE DE DECIS√ÉO")
print("-" * 21)
print(confusion_matrix(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))


import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Fun√ß√£o para gerar m√©tricas + matriz de confus√£o como string
def gerar_metricas(nome, modelo, X_test, y_test, y_pred):
    cm = confusion_matrix(y_test, y_pred)
    cm_str = f"[[{cm[0][0]}, {cm[0][1]}], [{cm[1][0]}, {cm[1][1]}]]"

    return {
        'Modelo': nome,
        'Acur√°cia': accuracy_score(y_test, y_pred),
        'Precis√£o': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred),
        'Matriz de Confus√£o': cm_str
    }

# Coletar m√©tricas de todos os modelos
resultados = [
    gerar_metricas("Regress√£o Log√≠stica", modelo_log, X_test_norm, y_test, y_pred_log),
    gerar_metricas("Random Forest", modelo_rf, X_test, y_test, y_pred_rf),
    gerar_metricas("KNN", modelo_knn, X_test_norm, y_test, y_pred_knn),
    gerar_metricas("√Årvore de Decis√£o", modelo_dt, X_test, y_test, y_pred_dt)
]

# Criar DataFrame
df_resultados = pd.DataFrame(resultados)
df_resultados = df_resultados.sort_values(by='F1-Score', ascending=False)

# Exibir a tabela
print("üìä COMPARATIVO DE MODELOS")
print("-" * 26)
display(df_resultados.round(4))


# Criar DataFrame com os dados fornecidos
dados_modelos = pd.DataFrame({
    'Modelo': ['Regress√£o Log√≠stica', 'Random Forest', 'KNN', '√Årvore de Decis√£o'],
    'Acur√°cia': [0.7996, 0.7896, 0.7569, 0.7264],
    'Precis√£o': [0.6544, 0.6434, 0.5473, 0.4859],
    'Recall': [0.5214, 0.4679, 0.4947, 0.5053],
    'F1-Score': [0.5804, 0.5418, 0.5197, 0.4954]
})

# Plotar gr√°fico de barras para cada m√©trica
dados_modelos.set_index('Modelo').plot(kind='bar', figsize=(12, 6), colormap='turbo')
plt.title('Comparativo de Desempenho dos Modelos')
plt.ylabel('Pontua√ß√£o')
plt.ylim(0, 1)
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.legend(title='M√©tricas')
plt.tight_layout()
plt.show()



# AN√ÅLISE DE IMPORT√ÇNCIA DAS VARI√ÅVEIS

# Extrair coeficientes do modelo treinado
coeficientes = modelo_log.coef_[0]

# Import√¢ncia: valor absoluto dos coeficientes
importancias = np.abs(coeficientes)

# Criar DataFrame com vari√°veis e suas import√¢ncias
df_importancias = pd.DataFrame({
    'Vari√°vel': X_train.columns,
    'Import√¢ncia': importancias
}).sort_values(by='Import√¢ncia', ascending=False)

# Plotar gr√°fico
plt.figure(figsize=(10, 6))
sns.barplot(
    x='Import√¢ncia',
    y='Vari√°vel',
    data=df_importancias,
    color='mediumpurple'
)
plt.title('Import√¢ncia das Vari√°veis - Regress√£o Log√≠stica')
plt.xlabel('Import√¢ncia - Coeficiente')
plt.ylabel('Vari√°veis')
plt.tight_layout()
plt.show()